
###BEST RESULT PARAMS, name="test"###

model_and_data_version=1

data_processor = DataProcessor(start_file_num=1, num_files=50, n_mfcc=20)

data_processor.choose_data_model(normalization_version=2, 
	data_version=model_and_data_version, 
	num_time_steps=101, k=10, percentile_test=0)

model = Model(data_processor, num_epochs=80)

model.train_and_predict(model_version=model_and_data_version, 
		save_model=True, N_MIXES=5, 
		model_name="./models/model_long_train.json", 
		weights_name="./models/model_long_train.h5", 
		input_data_start=0, num_preds=400, name="test")

def model_v1(self, N_MIXES=5):
	"""
	Train model!

	Version compatible with version 1 input/target/val method in DataProcessor
	"""
	self.N_MIXES = N_MIXES
	self.OUTPUT_DIMS = self.data_processor.target_data.shape[1]

	#MAYBE TRY WITHOUT RETURN_SEQUENCES????
	self.model = Sequential()
	self.model.add(LSTM(units=160, return_sequences=True,  
		input_shape=self.data_processor.input_data.shape[1:]))
	self.model.add(LSTM(units=160, return_sequences=True))
	self.model.add(LSTM(units=160))
	self.model.add(mdn.MDN(self.OUTPUT_DIMS, self.N_MIXES))
	self.model.compile(loss=mdn.get_mixture_loss_func(self.OUTPUT_DIMS, 
		self.N_MIXES), optimizer='nadam')
	self.model.fit(self.data_processor.input_data, 
		self.data_processor.target_data, epochs=self.num_epochs, 
		batch_size=64, validation_split=0.15, callbacks=[self.stats_cb])
	print(self.model.summary())




###BEST RESULT PARAMS, name="test_more_mfcc"###

model_and_data_version=1

data_processor = DataProcessor(start_file_num=1, num_files=10, n_mfcc=35)

data_processor.choose_data_model(normalization_version=2, 
	data_version=model_and_data_version, 
	num_time_steps=101, k=1, percentile_test=0)

model = Model(data_processor, num_epochs=80)

model.train_and_predict(model_version=model_and_data_version, 
		save_model=True, N_MIXES=5, 
		model_name="./models/model_long_train.json", 
		weights_name="./models/model_long_train.h5", 
		input_data_start=0, num_preds=400, name="test")

def model_v1(self, N_MIXES=5):
	"""
	Train model!

	Version compatible with version 1 input/target/val method in DataProcessor
	"""
	self.N_MIXES = N_MIXES
	self.OUTPUT_DIMS = self.data_processor.target_data.shape[1]

	#MAYBE TRY WITHOUT RETURN_SEQUENCES????
	self.model = Sequential()
	self.model.add(LSTM(units=160, return_sequences=True,  
		input_shape=self.data_processor.input_data.shape[1:]))
	self.model.add(LSTM(units=160, return_sequences=True))
	self.model.add(LSTM(units=160))
	self.model.add(mdn.MDN(self.OUTPUT_DIMS, self.N_MIXES))
	self.model.compile(loss=mdn.get_mixture_loss_func(self.OUTPUT_DIMS, 
		self.N_MIXES), optimizer='nadam')
	self.model.fit(self.data_processor.input_data, 
		self.data_processor.target_data, epochs=self.num_epochs, 
		batch_size=64, validation_split=0.15, callbacks=[self.stats_cb])
	print(self.model.summary())


###BEST RESULT PARAMS, name="BEST"###

model_and_data_version=2

data_processor = DataProcessor(start_file_num=1, num_files=10, n_mfcc=35)

data_processor.choose_data_model(normalization_version=2, 
	data_version=model_and_data_version, 
	num_time_steps=101, k=1, percentile_test=0)

model = Model(data_processor, num_epochs=80)

model.train_and_predict(model_version=model_and_data_version, 
		save_model=True, N_MIXES=5, 
		model_name="./models/model_long_train.json", 
		weights_name="./models/model_long_train.h5", 
		input_data_start=0, num_preds=400, name="test")

	def model_v1(self, N_MIXES=5):
		"""
		Train model!

		Version compatible with version 1 input/target/val method in DataProcessor
		"""
		self.N_MIXES = N_MIXES
		self.OUTPUT_DIMS = self.data_processor.target_data.shape[1]

		#MAYBE TRY WITHOUT RETURN_SEQUENCES????
		self.model = Sequential()
		self.model.add(LSTM(units=160, return_sequences=True,  
			input_shape=self.data_processor.input_data.shape[1:]))
		self.model.add(LSTM(units=160, return_sequences=True))
		self.model.add(LSTM(units=160))
		self.model.add(mdn.MDN(self.OUTPUT_DIMS, self.N_MIXES))
		self.model.compile(loss=mdn.get_mixture_loss_func(self.OUTPUT_DIMS, 
			self.N_MIXES), optimizer='nadam')
		self.model.fit(self.data_processor.input_data, 
			self.data_processor.target_data, epochs=self.num_epochs, 
			batch_size=64, validation_split=0.15, callbacks=[self.stats_cb])
		print(self.model.summary())



###BEST RESULT PARAMS, name="7hour_train"###


model_and_data_version=2

data_processor = DataProcessor(start_file_num=1, num_files=200, n_mfcc=35)

data_processor.choose_data_model(normalization_version=2, 
	data_version=model_and_data_version, 
	num_time_steps=201, k=20, percentile_test=0)

model = Model(data_processor, num_epochs=700)

model.train_and_predict(model_version=model_and_data_version, 
		save_model=True, N_MIXES=5, 
		model_name="./models/model_long_train.json", 
		weights_name="./models/model_long_train.h5", 
		input_data_start=0, num_preds=400, name="7hour_train")


	def model_v2(self, N_MIXES=5):
		self.N_MIXES = N_MIXES
		self.OUTPUT_DIMS = self.data_processor.target_data.shape[2]

		self.model = Sequential()
		self.model.add(LSTM(units=200, return_sequences=True,
			input_shape=self.data_processor.input_data.shape[1:]))
		#self.model.add(LSTM(units=150, kernel_regularizer=regularizers.l2(0.2), return_sequences=True))
		self.model.add(LSTM(units=200, return_sequences=True))
		self.model.add(LSTM(units=200, return_sequences=True))
		#self.model.add(LSTM(units=100, return_sequences=True))
		#self.model.add(BatchNormalization())
		self.model.add(TimeDistributed(mdn.MDN(self.OUTPUT_DIMS, self.N_MIXES)))

		#sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True, clipnorm=1.)
		#adam = keras.optimizers.Adam(lr=0.00001, beta_1=0.9, beta_2=0.999, epsilon=1e-8)
		self.model.compile(loss=mdn.get_mixture_loss_func(self.OUTPUT_DIMS, 
			self.N_MIXES), optimizer='nadam')
		history = self.model.fit(self.data_processor.input_data, 
			self.data_processor.target_data, epochs=self.num_epochs, 
			batch_size=64, validation_split=0.15, callbacks=[self.stats_cb])
		print(self.model.summary())




###BEST RESULT PARAMS, name="BEST2"###

model_and_data_version=2
name = "BEST2"

data_processor = DataProcessor(start_file_num=1, num_files=10, n_mfcc=35)

data_processor.choose_data_model(normalization_version=2, 
	data_version=model_and_data_version, 
	num_time_steps=101, k=1, percentile_test=0)

model = Model(data_processor, num_epochs=100)

model.train_and_predict(model_version=model_and_data_version, 
		save_model=True, N_MIXES=5, 
		model_name="./models/{}.json".format(name), 
		weights_name="./models/{}.h5".format(name), 
		input_data_start=0, num_preds=400, name=name)


	def model_v2(self, N_MIXES=5, name="default2"):
		"""
		Train model!

		Version compatible with version 2 AND 3 input/target/val method in DataProcessor
		"""
		self.N_MIXES = N_MIXES
		self.OUTPUT_DIMS = self.data_processor.target_data.shape[2]

		self.model = Sequential()
		self.model.add(LSTM(units=200, return_sequences=True,
			input_shape=self.data_processor.input_data.shape[1:]))
		#self.model.add(LSTM(units=150, kernel_regularizer=regularizers.l2(0.2), return_sequences=True))
		self.model.add(LSTM(units=200, return_sequences=True))
		self.model.add(LSTM(units=200, return_sequences=True))
		#self.model.add(LSTM(units=100, return_sequences=True))
		#self.model.add(BatchNormalization())
		self.model.add(TimeDistributed(mdn.MDN(self.OUTPUT_DIMS, self.N_MIXES)))

		#sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True, clipnorm=1.)
		#adam = keras.optimizers.Adam(lr=0.00001, beta_1=0.9, beta_2=0.999, epsilon=1e-8)
		self.model.compile(loss=mdn.get_mixture_loss_func(self.OUTPUT_DIMS, 
			self.N_MIXES), optimizer='nadam')
		history = self.model.fit(self.data_processor.input_data, 
			self.data_processor.target_data, epochs=self.num_epochs, 
			batch_size=64, validation_split=0.15, callbacks=[self.stats_cb])



###BEST RESULT PARAMS, name="7hour_train2"###

model_and_data_version=2
name = "7hour_train2"

data_processor = DataProcessor(start_file_num=1, num_files=200, n_mfcc=35)

data_processor.choose_data_model(normalization_version=2, 
	data_version=model_and_data_version, 
	num_time_steps=201, k=20, percentile_test=0)

model = Model(data_processor, num_epochs=750)

model.train_and_predict(model_version=model_and_data_version, 
		save_model=True, N_MIXES=5, 
		model_name="./models/{}.json".format(name), 
		weights_name="./models/{}.h5".format(name), 
		input_data_start=0, num_preds=400, name=name)


	def model_v2(self, N_MIXES=5, name="default2"):
		"""
		Train model!

		Version compatible with version 2 AND 3 input/target/val method in DataProcessor
		"""
		self.N_MIXES = N_MIXES
		self.OUTPUT_DIMS = self.data_processor.target_data.shape[2]

		self.model = Sequential()
		self.model.add(LSTM(units=220, return_sequences=True,
			input_shape=self.data_processor.input_data.shape[1:]))
		#self.model.add(LSTM(units=150, kernel_regularizer=regularizers.l2(0.2), return_sequences=True))
		self.model.add(LSTM(units=220, return_sequences=True))
		self.model.add(LSTM(units=220, return_sequences=True))
		#self.model.add(LSTM(units=100, return_sequences=True))
		#self.model.add(BatchNormalization())
		self.model.add(TimeDistributed(mdn.MDN(self.OUTPUT_DIMS, self.N_MIXES)))

		#sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True, clipnorm=1.)
		#adam = keras.optimizers.Adam(lr=0.00001, beta_1=0.9, beta_2=0.999, epsilon=1e-8)
		self.model.compile(loss=mdn.get_mixture_loss_func(self.OUTPUT_DIMS, 
			self.N_MIXES), optimizer='nadam')
		history = self.model.fit(self.data_processor.input_data, 
			self.data_processor.target_data, epochs=self.num_epochs, 
			batch_size=64, validation_split=0.15, callbacks=[self.stats_cb])

